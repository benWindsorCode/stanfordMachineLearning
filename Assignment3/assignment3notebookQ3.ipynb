{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS229 Assignment 3 - First Coding Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing the $l_1$-regularised least squares algorithm from question 3 of problem set 3 (https://see.stanford.edu/materials/aimlcs229/problemset3.pdf ) as part of Stanford's Engineering Everywhere CS229 Machine Learning course, taught by Andrew Ng in '08 ( https://see.stanford.edu/course/cs229 )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question we will be minimising the cost function $$J(\\theta)=\\frac{1}{2}||X\\theta - \\vec{y}||_2^2 + \\lambda ||\\theta||_1,$$ where $X$ is our matrix of training examples, $\\vec{y}$ is the vector of the correct classifications and $\\theta$ is the parameter vector for our algorithm to learn. The $\\lambda$ parameter contorls the amount of penilisation we want to apply. \n",
    "\n",
    "(For those who haven't studied it yet, the $||\\cdot||_n$ is called a norm and is a way to measure the size of a vector in an abstract sense. Really what we care about here is the sum of the squared values of a vector, or the sum of the absolute values of a vector. Checkout for more details: https://machinelearningmastery.com/vector-norms-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading in our data. Note that $X$ is a $20\\times 100$ sized matrix, and $Y$ is a $20\\times 1$ vector, so that means we have a $100$ input features, with $20$ training pairs. Hence we have $\\theta\\in\\mathbb{R}^{100}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "xData=np.asmatrix(np.loadtxt(\"data/q3/x.dat\"))\n",
    "yData = np.transpose(np.asmatrix(np.loadtxt(\"data/q3/y.dat\")))\n",
    "m=xData.shape[0] # number of training examples\n",
    "n=xData.shape[1] # number of features\n",
    "converganceConst=0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to implement our cost function $J$. This is what we are aiming to minimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(X, y, theta, lambdaConst):\n",
    "    return 0.5*np.linalg.norm(X*theta-y)**2+lambdaConst*np.sum(np.absolute(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimise we perform coordinate descent on $J$. To perform the coordinate descent we need to be able to calculate its derivative and set it to $0$. Unfortunately this is difficult when we have the $||\\theta||_1=\\sum|\\theta_i|$ term there as this is not-differentiable in any of the $\\theta_i$. \n",
    "\n",
    "The assignment has us rewrite $J$ cleverly to allow us to perform this differentiation, by extracting out the $\\theta_i$ term and replacing the modulus with multiplication by sign($\\theta_i$). Once we have done this, we perform the differentiation, and we see that we get $\\theta_i$ updates to the following quantity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_i = \\frac{-X_i^T(X\\bar{\\theta}-\\vec{y})-\\lambda s_i}{X_i^TX_i},$$\n",
    "where $X_i$ is the $i$th column of $X$, $\\bar{\\theta}$ is $\\theta$ with the $i$th entry set to $0$ and $s_i$ is the sign of $\\theta_i$. We first write a function to update a single $\\theta_i$. This comes with the caveat that we have to take the max/min with $0$ and see which version then gives the smallest objective function to account for the introduction of the $s_i$ sign we did. (See the assignment solutions for more details on the maths here: https://see.stanford.edu/materials/aimlcs229/ps3_solution.pdf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateSingleTheta(i, X, y, theta, lambdaConst):\n",
    "    Xi=X[:,i]\n",
    "    theta1, theta2, thetaBar=np.copy(theta), np.copy(theta), np.copy(theta)\n",
    "    thetaBar[i]=0\n",
    "    numerator1=-np.transpose(Xi)*(X*thetaBar-y)-lambdaConst\n",
    "    numerator2=-np.transpose(Xi)*(X*thetaBar-y)+lambdaConst\n",
    "    denominator=np.transpose(Xi)*Xi\n",
    "    theta_i1=np.maximum(numerator1/denominator, 0)\n",
    "    theta_i2=np.minimum(numerator2/denominator, 0)\n",
    "    theta1[i]=theta_i1\n",
    "    theta2[i]=theta_i2\n",
    "    objective1=objective(X,y,theta1,lambdaConst)\n",
    "    objective2=objective(X,y,theta2,lambdaConst)\n",
    "    if (objective1 < objective2):\n",
    "        return theta_i1\n",
    "    else:\n",
    "        return theta_i2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine these individual update steps together to update the whole $\\theta$ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTheta(X, y, theta, lambdaConst):\n",
    "    diffs=[]\n",
    "    for i in range(n):\n",
    "        oldThetai=np.copy(theta[i])\n",
    "        theta[i]=updateSingleTheta(i, X, y, theta, lambdaConst)\n",
    "        diffs.append(abs(oldThetai-theta[i]))\n",
    "    maxDiff=np.amax(diffs)\n",
    "    return theta, maxDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to now write a function to repeat this updating until our $\\theta$ converges. From the assgnment we are told to stop once all of the coordinates in one pass change by less than $10^{-5}$. We do this as part of the l1ls(X, y, lambda) function we are asked to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1ls(X, y, lambdaConst):\n",
    "    iterate=True\n",
    "    theta=np.ones((n,1))\n",
    "    while(iterate):\n",
    "        theta, maxDiff=updateTheta(X, y, theta, lambdaConst)\n",
    "        if maxDiff < converganceConst:\n",
    "            iterate=False\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement our hypothesis function, given a column vector $\\theta$ of parameters and a column vector $x$ of input, we output $h_\\theta(X)=\\theta^T X = \\theta_1x_1+\\cdots+\\theta_nx_n$ as our prediction. This is not used in the solution directly but it allows you to play with the predictions so I wanted to include it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    return np.asscalar(np.transpose(theta)*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we do as the assignment says and can run our algorithm for different $\\lambda$ parameters. In particular the goal of this quesiton is to notice that when we set $\\lambda=1$ we see that our predicted $\\theta$ matches the sparsity pattern of the true $\\theta$. i.e. it tells us exactly which features in the true $\\theta$ were non-zero. In particular we could now use this as a feature identification algorithm. (You'll have to scroll a little here, sorry!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49875625]\n",
      " [ 0.65610659]\n",
      " [-0.79057697]\n",
      " [-0.65564318]\n",
      " [-0.89191725]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "theta = l1ls(xData, yData, 1)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.68372018]\n",
      " [ 0.84110202]\n",
      " [-0.83028605]\n",
      " [-0.85031124]\n",
      " [-0.93904984]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]\n",
      " [ 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "trueTheta = np.transpose(np.asmatrix(np.loadtxt(\"data/q3/theta.dat\")))\n",
    "print(trueTheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
